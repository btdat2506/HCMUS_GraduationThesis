% Generated by IEEEtran.bst, version: 1.12 (2007/01/11)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{moreau2018}
T.~Moreau \emph{et~al.}, ``{VTA: An Open Hardware-Software Stack for Deep
  Learning},'' \emph{CoRR}, 2018.

\bibitem{venkatesan2019magnet}
R.~Venkatesan \emph{et~al.}, ``{MAGNet: A Modular Accelerator Generator for
  Neural Networks},'' in \emph{ICCAD}, 2019.

\bibitem{polysa}
J.~Cong \emph{et~al.}, ``{PolySA: polyhedral-based systolic array
  auto-compilation},'' in \emph{ICCAD}, 2018.

\bibitem{zhang2018}
X.~Zhang \emph{et~al.}, ``{DNNBuilder: An Automated Tool for Building
  High-performance DNN Hardware Accelerators for FPGAs},'' in \emph{ICCAD},
  2018.

\bibitem{automated-systolic-cnn-fpgas}
{Xuechao Wei} \emph{et~al.}, ``Automated systolic array architecture synthesis
  for high throughput cnn inference on fpgas,'' in \emph{DAC}, 2017.

\bibitem{deepburning}
Y.~{Wang} \emph{et~al.}, ``Deepburning: Automatic generation of fpga-based
  learning accelerators for the neural network family,'' in \emph{DAC}, 2016.

\bibitem{hybrid-dnn}
H.~{Ye} \emph{et~al.}, ``{HybridDNN}: A framework for high-performance hybrid
  dnn accelerator design and implementation,'' in \emph{DAC}, 2020.

\bibitem{datacenter-facebook}
K.~Hazelwood \emph{et~al.}, ``{Applied Machine Learning at Facebook: A
  Datacenter Infrastructure Perspective},'' in \emph{HPCA}, 2018.

\bibitem{edge-facebook}
C.-J. Wu \emph{et~al.}, ``{Machine Learning at Facebook: Understanding
  Inference at the Edge},'' in \emph{HPCA}, 2019.

\bibitem{ai-tax-hpca}
D.~{Richins} \emph{et~al.}, ``{Missing the Forest for the Trees: End-to-End AI
  Application Performance in Edge Data Centers},'' in \emph{HPCA}, 2020.

\bibitem{nvdla-hotchips}
F.~Sijstermans, ``{The NVIDIA Deep Learning Accelerator},'' in \emph{Hot
  Chips}, 2018.

\bibitem{eyeriss2}
Y.~{Chen} \emph{et~al.}, ``{Eyeriss v2: A Flexible Accelerator for Emerging
  Deep Neural Networks on Mobile Devices},'' \emph{JETCAS}, 2019.

\bibitem{shidiannao}
Z.~{Du} \emph{et~al.}, ``{ShiDianNao: Shifting vision processing closer to the
  sensor},'' in \emph{ISCA}, 2015.

\bibitem{scaledeep}
S.~Venkataramani \emph{et~al.}, ``{ScaleDeep: A Scalable Compute Architecture
  for Learning and Evaluating Deep Networks},'' in \emph{ISCA}, 2017.

\bibitem{interstellar-asplos2020}
X.~Yang \emph{et~al.}, ``Interstellar: Using {Halide}â€™s scheduling language
  to analyze {DNN} accelerators,'' in \emph{ASPLOS}, 2020.

\bibitem{maeri-asplos2018}
H.~Kwon \emph{et~al.}, ``{MAERI: Enabling Flexible Dataflow Mapping over {DNN}
  Accelerators via Programmable Interconnects},'' in \emph{ASPLOS}, 2018.

\bibitem{brainwave-isca-2018}
J.~Fowers \emph{et~al.}, ``{A Configurable Cloud-Scale DNN Processor for
  Real-Time AI},'' in \emph{ISCA}, 2018.

\bibitem{sigma-hpca2020}
E.~{Qin} \emph{et~al.}, ``Sigma: A sparse and irregular gemm accelerator with
  flexible interconnects for dnn training,'' in \emph{HPCA}, 2020.

\bibitem{sparse-tpu}
X.~He \emph{et~al.}, ``Sparse-{TPU}: Adapting systolic arrays for sparse
  matrices,'' in \emph{ICS}, 2020.

\bibitem{sparse-train}
P.~{Dai} \emph{et~al.}, ``{SparseTrain}: Exploiting dataflow sparsity for
  efficient convolutional neural networks training,'' in \emph{DAC}, 2020.

\bibitem{pipelayer}
L.~Song \emph{et~al.}, ``{PipeLayer: A Pipelined ReRAM-Based Accelerator for
  Deep Learning},'' in \emph{HPCA}, 2017.

\bibitem{algo-hardware-codesign-for-in-memory}
H.~{Kim} \emph{et~al.}, ``Algorithm/hardware co-design for in-memory neural
  network computing with minimal peripheral circuit overhead,'' in \emph{DAC},
  2020.

\bibitem{dnnweaver}
H.~Sharma \emph{et~al.}, ``{From High-level Deep Neural Models to FPGAs},'' in
  \emph{MICRO}, 2016.

\bibitem{centaur-isca2020}
G.~{Henry} \emph{et~al.}, ``{High-Performance Deep-Learning Coprocessor
  Integrated into x86 SoC with Server-Class CPUs Industrial Product},'' in
  \emph{ISCA}, 2020.

\bibitem{lustig2013tlb}
D.~Lustig \emph{et~al.}, ``Tlb improvements for chip multiprocessors:
  Inter-core cooperative prefetchers and shared last-level tlbs,'' \emph{TACO},
  2013.

\bibitem{chipyard}
A.~Amid \emph{et~al.}, ``{Chipyard: Integrated Design, Simulation, and
  Implementation Framework for Custom SoCs},'' \emph{IEEE Micro}, 2020.

\bibitem{karandikar2018firesim}
S.~{Karandikar} \emph{et~al.}, ``{FireSim: FPGA-Accelerated Cycle-Exact
  Scale-Out System Simulation in the Public Cloud},'' in \emph{ISCA}, 2018.

\bibitem{neummu-asplos2020}
B.~Hyun \emph{et~al.}, ``{NeuMMU: Architectural Support for Efficient Address
  Translations in Neural Processing Units},'' in \emph{ASPLOS}, 2020.

\bibitem{address-trans-cong-hpca2017}
Y.~{Hao} \emph{et~al.}, ``{Supporting Address Translation for
  Accelerator-Centric Architectures},'' in \emph{HPCA}, 2017.

\end{thebibliography}
